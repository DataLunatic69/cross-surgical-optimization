model:
  name: "medalpaca/medalpaca-7b"  # Medical-specific LLM
  quantization: 4  # 4-bit quantization for efficiency
  gradient_checkpointing: true
  cache_dir: "./data/models"
  
  lora:
    peft_lora_r: 16
    peft_lora_alpha: 32
    target_modules:
      - "q_proj"
      - "k_proj" 
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    lora_dropout: 0.075
    task_type: "CAUSAL_LM"

train:
  learning_rate_max: 0.0002
  learning_rate_min: 0.00001
  save_every_round: 2
  seq_length: 512
  
  training_arguments:
    output_dir: "./data/checkpoints"
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 4
    num_train_epochs: 1
    logging_steps: 10
    save_strategy: "epoch"
    evaluation_strategy: "no"
    fp16: true
    gradient_checkpointing: true
    report_to: "none"
    remove_unused_columns: false
    
strategy:
  fraction_fit: 1.0
  fraction_evaluate: 1.0
  min_fit_clients: 2
  min_evaluate_clients: 2
  min_available_clients: 2

static:
  dataset:
    name: "surgical_outcomes"  # Custom dataset name